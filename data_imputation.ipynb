{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data imputation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # Load the median values for imputation\n",
    "# sesame_450k_median = pd.read_csv(\"/tank/projects/vpalagina_hobotnica/hobotnica/sesame_450k_median.csv\", index_col=0)\n",
    "\n",
    "# data = pd.read_parquet(\"/tank/projects/vpalagina_hobotnica/hobotnica/data with HC/GSE32148.parquet\")\n",
    "# data.index = data[\"index\"]\n",
    "# data = data.drop([\"index\"], axis=1)\n",
    "# imputation_values = sesame_450k_median.loc[data.index]\n",
    "\n",
    "# # methylation_data_imputed.to_csv('methylation_data_imputed.csv')\n",
    "# for col in data.columns:\n",
    "#     data[col] = data[col].fillna(imputation_values[\"median\"])\n",
    "# data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation complete for all files.\n"
     ]
    }
   ],
   "source": [
    "#Imputation. Here I check the sum of NAs in the samples, if more than half, I delite such sites\n",
    "#If less than half, I impute from reference(if site is present in it) or calculate the median across samples of\n",
    "#this exact site and replace with median\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the directory containing the Parquet files\n",
    "directory_path = \"/tank/projects/vpalagina_hobotnica/hobotnica/data with HC\"\n",
    "\n",
    "# Load the median values for imputation from the CSV file\n",
    "sesame_450k_median = pd.read_csv(\"/tank/projects/vpalagina_hobotnica/hobotnica/sesame_450k_median.csv\", index_col=0)\n",
    "\n",
    "output_directory  = \"/tank/projects/vpalagina_hobotnica/hobotnica/data_with_HC_imputed\"\n",
    "\n",
    "# Iterate over each Parquet file in the directory\n",
    "for filename in os.listdir(directory_path)[20:]:\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    \n",
    "    # Load the Parquet file\n",
    "    data = pd.read_parquet(file_path)\n",
    "    \n",
    "    if data.columns[0] != \"index\":\n",
    "    # Rename the first column to \"index\"\n",
    "        data.rename(columns={data.columns[0]: \"index\"}, inplace=True)\n",
    "        \n",
    "    # Calculate the threshold for maximum allowable NAs in a row (half of the columns)\n",
    "    na_threshold = len(data.columns)*0.9\n",
    "    \n",
    "    # Identify rows with more than 90% columns as NaN and remove them\n",
    "    data = data[data.isna().sum(axis=1) <= na_threshold]\n",
    "    \n",
    "    # Identify common indices between data and median values\n",
    "    common_indices = data[\"index\"].values\n",
    "    common_indices = pd.Index(common_indices).intersection(sesame_450k_median.index)\n",
    "    \n",
    "    # Align the imputation values with the data index\n",
    "    imputation_values = sesame_450k_median.loc[common_indices, \"median\"]\n",
    "    \n",
    "    # Set the index to \"index\" column for alignment\n",
    "    data.set_index(\"index\", inplace=True)\n",
    "    \n",
    "    # Perform imputation for common indices using sesame median values\n",
    "    for col in data.columns:\n",
    "        data.loc[common_indices, col] = data.loc[common_indices, col].fillna(imputation_values)\n",
    "    \n",
    "   # Handle rows not present in the reference file\n",
    "    non_common_indices = data.index.difference(common_indices)\n",
    "    \n",
    "    # Fill NAs in remaining rows with their row medians\n",
    "    for index in non_common_indices:\n",
    "        data.loc[index] = data.loc[index].fillna(data.loc[index].median())\n",
    "    \n",
    "    # Reset index to move \"index\" back to a column\n",
    "    data.reset_index(inplace=True)\n",
    "    \n",
    "    # Save the imputed DataFrame to the output directory\n",
    "    output_file_path = os.path.join(output_directory, filename)\n",
    "    data.to_parquet(output_file_path)\n",
    "\n",
    "print(\"Imputation complete for all files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed GSE56046.parquet: Removed 0 rows with NA values\n",
      "Processed GSE42861.parquet: Removed 0 rows with NA values\n",
      "Processed GSE111629.parquet: Removed 869 rows with NA values\n",
      "Processed GSE32148.parquet: Removed 19576 rows with NA values\n",
      "Processed GSE87640.parquet: Removed 0 rows with NA values\n",
      "Processed GSE87648.parquet: Removed 0 rows with NA values\n",
      "Processed GSE72338.parquet: Removed 33919 rows with NA values\n",
      "Processed GSE217633.parquet: Removed 0 rows with NA values\n",
      "Processed GSE214297.parquet: Removed 3272 rows with NA values\n",
      "Processed GSE134429.parquet: Removed 0 rows with NA values\n",
      "Processed GSE81961.parquet: Removed 52 rows with NA values\n",
      "Processed GSE77696.parquet: Removed 10 rows with NA values\n",
      "Processed GSE67705.parquet: Removed 0 rows with NA values\n",
      "Processed GSE182991.parquet: Removed 3272 rows with NA values\n",
      "Processed GSE49909.parquet: Removed 548 rows with NA values\n",
      "Processed GSE175364.parquet: Removed 446722 rows with NA values\n",
      "Processed GSE166611.parquet: Removed 2038 rows with NA values\n",
      "Processed GSE143942.parquet: Removed 0 rows with NA values\n",
      "Processed GSE56581.parquet: Removed 0 rows with NA values\n",
      "Processed GSE72776.parquet: Removed 131 rows with NA values\n",
      "Processed GSE122244.parquet: Removed 128270 rows with NA values\n",
      "Processed GSE193836.parquet: Removed 0 rows with NA values\n",
      "Processed GSE219293.parquet: Removed 0 rows with NA values\n",
      "Processed GSE118469.parquet: Removed 6408 rows with NA values\n",
      "Processed GSE145714.parquet: Removed 838 rows with NA values\n",
      "Processed GSE107143.parquet: Removed 0 rows with NA values\n",
      "Processed GSE131752.parquet: Removed 0 rows with NA values\n",
      "Processed GSE156994.parquet: Removed 2712 rows with NA values\n",
      "Processed GSE72774.parquet: Removed 839 rows with NA values\n",
      "Processed GSE131989.parquet: Removed 151 rows with NA values\n",
      "Processed GSE144858.parquet: Removed 0 rows with NA values\n",
      "Processed GSE130030.parquet: Removed 0 rows with NA values\n",
      "Processed GSE67751.parquet: Removed 324 rows with NA values\n",
      "Processed GSE59685.parquet: Removed 0 rows with NA values\n",
      "Processed GSE106648.parquet: Removed 1942 rows with NA values\n",
      "Processed GSE130029.parquet: Removed 0 rows with NA values\n",
      "Processed GSE118468.parquet: Removed 6734 rows with NA values\n",
      "Processed GSE56606.parquet: Removed 1319 rows with NA values\n",
      "Processed GSE99624.parquet: Removed 20656 rows with NA values\n",
      "Processed GSE71841.parquet: Removed 0 rows with NA values\n",
      "Processed GSE111223.parquet: Removed 53943 rows with NA values\n",
      "All datasets processed.\n"
     ]
    }
   ],
   "source": [
    "#Removing of NAs values in the column\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the datasets\n",
    "folder_path = '/tank/projects/vpalagina_hobotnica/hobotnica/data with HC'\n",
    "output = \"/tank/projects/vpalagina_hobotnica/hobotnica/data_with_HC_removed_NA\"\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Check for NA values and remove rows with any NA values\n",
    "    initial_row_count = len(df)\n",
    "    df_cleaned = df.dropna()\n",
    "    final_row_count = len(df_cleaned)\n",
    "    \n",
    "    # Save the cleaned dataset back to a new file (optional)\n",
    "    cleaned_file_path = os.path.join(output, filename)\n",
    "    df_cleaned.to_parquet(cleaned_file_path)\n",
    "\n",
    "    print(f\"Processed {filename}: Removed {initial_row_count - final_row_count} rows with NA values\")\n",
    "\n",
    "print(\"All datasets processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
